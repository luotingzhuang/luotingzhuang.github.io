
@article{zhuang_advancing_2025,
	title = {Advancing {Precision} {Oncology} {Through} {Modeling} of {Longitudinal} and {Multimodal} {Data}},
	issn = {1941-1189},
	url = {https://ieeexplore.ieee.org/abstract/document/11068945},
	doi = {10.1109/RBME.2025.3577587},
	abstract = {Cancer evolves continuously over time through a complex interplay of genetic, epigenetic, microenvironmental, and phenotypic changes. This dynamic behavior drives uncontrolled cell growth, metastasis, immune evasion, and therapy resistance, posing challenges for effective monitoring and treatment. However, today’s data-driven research in oncology has primarily focused on cross-sectional analysis using data from a single modality, limiting the ability to fully characterize and interpret the disease’s dynamic heterogeneity. Advances in multiscale data collection and computational methods now enable the discovery of longitudinal multimodal biomarkers for precision oncology. Longitudinal data reveal patterns of disease progression and treatment response that are not evident from single-timepoint data, enabling timely abnormality detection and dynamic treatment adaptation. Multimodal data integration offers complementary information from diverse sources for more precise risk assessment and targeting of cancer therapy. In this review, we survey methods of longitudinal and multimodal modeling, highlighting their synergy in providing multifaceted insights for personalized care tailored to the unique characteristics of a patient’s cancer. We summarize the current challenges and future directions of longitudinal multimodal analysis in advancing precision oncology.},
	urldate = {2025-08-08},
	journal = {IEEE Reviews in Biomedical Engineering},
	author = {Zhuang, Luoting and Park, Stephen H. and Skates, Steven J. and Prosper, Ashley E. and Aberle, Denise R. and Hsu, William},
	year = {2025},
	keywords = {Artificial intelligence, Biomarkers, Biomedical imaging, Cancer, cancer biomarkers, Data models, Histopathology, Imaging, Immune system, longitudinal modeling, Medical services, multimodal fusion, Oncology, precision oncology, Tumors},
	pages = {1--19},
	file = {Full Text PDF:/Users/luotingzhuang/Zotero/storage/I8B25F4B/Zhuang et al. - 2025 - Advancing Precision Oncology Through Modeling of L.pdf:application/pdf},
}

@misc{zhuang_vision-language_2025,
	title = {Vision-{Language} {Model}-{Based} {Semantic}-{Guided} {Imaging} {Biomarker} for {Early} {Lung} {Cancer} {Detection}},
	url = {http://arxiv.org/abs/2504.21344},
	doi = {10.48550/arXiv.2504.21344},
	abstract = {Objective: A number of machine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. However, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. Thus, this research aims to integrate semantic features derived from radiologists' assessments of nodules, allowing the model to learn clinically relevant, robust, and explainable features for predicting lung cancer. Methods: We obtained 938 low-dose CT scans from the National Lung Screening Trial with 1,246 nodules and semantic features. The Lung Image Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for nodule characteristics. Three external datasets were obtained from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We finetuned a pretrained Contrastive Language-Image Pretraining model with a parameter-efficient fine-tuning approach to align imaging and semantic features and predict the one-year lung cancer diagnosis. Results: We evaluated the performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and compared it to three state-of-the-art models. Our model demonstrated an AUROC of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on external datasets. Using CLIP, we also obtained predictions on semantic features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and pleural attachment (0.84), that can be used to explain model predictions. Conclusion: Our approach accurately classifies lung nodules as benign or malignant, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. This approach also prevents the model from learning shortcuts and generalizes across clinical settings.},
	urldate = {2025-08-08},
	publisher = {arXiv},
	author = {Zhuang, Luoting and Tabatabaei, Seyed Mohammad Hossein and Salehi-Rad, Ramin and Tran, Linh M. and Aberle, Denise R. and Prosper, Ashley E. and Hsu, William},
	month = apr,
	year = {2025},
	note = {arXiv:2504.21344 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Quantitative Methods},
	file = {Preprint PDF:/Users/luotingzhuang/Zotero/storage/VEY3XGZJ/Zhuang et al. - 2025 - Vision-Language Model-Based Semantic-Guided Imagin.pdf:application/pdf;Snapshot:/Users/luotingzhuang/Zotero/storage/N56VUDYN/2504.html:text/html},
}

@inproceedings{zhuang_enhancing_2025,
	title = {Enhancing {Lung} {Segmentation} {Algorithms} to {Ensure} {Inclusion} of {Juxtapleural} {Nodules}},
	url = {https://ieeexplore.ieee.org/abstract/document/10981085},
	doi = {10.1109/ISBI60581.2025.10981085},
	abstract = {Computed tomography (CT) is pivotal in detecting and monitoring pulmonary nodules in lung cancer screening. With the advancement of artificial intelligence in medical imaging, accurate lung segmentation has become crucial for reliable feature extraction. While traditional methods lack generalizability, deep learning models also encounter difficulties including juxtapleural nodules. To overcome the challenge, we finetuned a 3D U-Net by randomly masking out 70\% of the images, which forces the model to infer the missing regions and learn the boundaries of the lungs. Our model achieved a Dice of 0.982 in lung segmentation. Notably, our approach achieved higher sensitivity compared to three state-of-the-art deep learning models in the inclusion of juxtapleural and large masses by 0.11, 0.20, and 0.52, respectively. Additionally, it consistently outperformed these models on external datasets. The improved result in nodule inclusion allows for more accurate and robust downstream analysis and computer-aided diagnosis of lung cancer. Our model also provided pixel-level uncertainty estimates, visually presenting where the model is confident or uncertain. High-uncertainty areas can be flagged for further examination by both clinicians and researchers. Our implementation is available at https://github.com/luotingzhuang/maskedSeg.},
	urldate = {2025-08-08},
	booktitle = {2025 {IEEE} 22nd {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Zhuang, Luoting and Tabatabaei, Seyed Mohammad Hossein and Prosper, Ashley E. and Hsu, William},
	month = apr,
	year = {2025},
	note = {ISSN: 1945-8452},
	keywords = {Accuracy, Biomedical imaging, Computational modeling, Computed tomography, Computed Tomography, Deep learning, Deep Learning, Image segmentation, Juxtapleural Nodules, Lung cancer, Lung Segmentation, Lungs, Masked Encoder, Reliability, Uncertainty, Uncertainty Estimates},
	pages = {1--5},
	file = {Full Text PDF:/Users/luotingzhuang/Zotero/storage/TERNDUPD/Zhuang et al. - 2025 - Enhancing Lung Segmentation Algorithms to Ensure I.pdf:application/pdf},
}

@inproceedings{zhuang_comparing_2025,
	title = {Comparing the characteristics and robustness of imaging features via prompt selection in generalist segmentation models},
	volume = {13411},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13411/134110Z/Comparing-the-characteristics-and-robustness-of-imaging-features-via-prompt/10.1117/12.3049090.full},
	doi = {10.1117/12.3049090},
	abstract = {Machine learning models have been developed on computed tomography (CT) scans to advance lung cancer diagnosis and prognosis. While various types of features are extracted from CT scans, the nature and robustness of these features are still not well understood. These imaging features are challenging to interpret as the information they capture is often unclear. Here, we aim to investigate different imaging features by employing a novel method: selecting prompts based on feature similarity for nodule segmentation. We utilize two generalist foundation models, SegGPT and SAM2, which leverage in-context information from a pair of prompt images and segmentation guides. This approach enables the models to execute an out-of-domain segmentation task effectively. However, their effectiveness depends on the prompt example, performing better when prompt and test images share similar semantics, backgrounds, and appearances. We used four features for prompt selection: pixel intensity, radiomics features, imaging biomarker foundation features, and fine-tuned Contrastive Language-Image Pre-Training (CLIP) features. Among the features examined, CLIP features-based prompt selection shows superior segmentation accuracy and robustness on external datasets by focusing more effectively on nodule characteristics. The combination of foundation and CLIP features provides complementary benefits and enhances segmentation performance: foundation features effectively identify prompts with similar backgrounds, while CLIP features excel at finding prompts with similar nodule characteristics. Our findings provide valuable insights into commonly used imaging features, helping researchers select the most appropriate features for specific prediction tasks.},
	urldate = {2025-08-08},
	booktitle = {Medical {Imaging} 2025: {Imaging} {Informatics}},
	publisher = {SPIE},
	author = {Zhuang, Luoting and Tabatabaei, Seyed Mohammad Hossein and Aberle, Denise R. and Prosper, Ashley E. and Hsu, William},
	month = apr,
	year = {2025},
	pages = {220--226},
	file = {Full Text PDF:/Users/luotingzhuang/Zotero/storage/7D7C8AHK/Zhuang et al. - 2025 - Comparing the characteristics and robustness of im.pdf:application/pdf},
}

@misc{xue_smokebert_2025,
	title = {{SmokeBERT}: {A} {BERT}-based {Model} for {Quantitative} {Smoking} {History} {Extraction} from {Clinical} {Narratives} to {Improve} {Lung} {Cancer} {Screening}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{SmokeBERT}},
	url = {https://www.medrxiv.org/content/10.1101/2025.06.18.25329870v1},
	doi = {10.1101/2025.06.18.25329870},
	abstract = {Tobacco use is a critical risk factor for diseases such as cancer and cardiovascular disorders. While electronic health records can capture categorical smoking statuses accurately, granular quantitative details, such as pack years and years since quitting, are often embedded in clinical narratives. This information is crucial for assessing disease risk and determining eligibility for lung cancer screening (LCS). Existing natural language processing (NLP) tools excelled at identifying smoking statuses but struggled with extracting detailed quantitative data. To address this, we developed SmokeBERT, a fine-tuned BERT-based model optimized for extracting detailed smoking histories. Evaluations against a state-of-the-art rule-based NLP model demonstrated its superior performance on F1 scores (0.97 vs. 0.88 on the hold-out test set) and identification of LCS-eligible patients (e.g., 98\% vs. 60\% for ≥20 pack years). Future work includes creating a multilingual, language-agnostic version of SmokeBERT by incorporating datasets in multiple languages, exploring ensemble methods, and testing on larger datasets.},
	language = {en},
	urldate = {2025-08-08},
	publisher = {medRxiv},
	author = {Xue, Yiming and Zhu, Yunzheng and Zhuang, Luoting and Oh, YongKyung and Taira, Ricky and Aberle, Denise R. and Prosper, Ashley E. and Hsu, William and Lin, Yannan},
	month = jun,
	year = {2025},
	note = {Pages: 2025.06.18.25329870},
	file = {Full Text PDF:/Users/luotingzhuang/Zotero/storage/WZHWYW2M/Xue et al. - 2025 - SmokeBERT A BERT-based Model for Quantitative Smo.pdf:application/pdf},
}

@inproceedings{zhuang_exploring_2024,
	title = {Exploring the {Impact} of {Acquisition} and {Reconstruction} {Parameters} on an {Imaging}-{Based} {Lung} {Cancer} {Risk} {Model}},
	url = {https://ieeexplore.ieee.org/abstract/document/10781833},
	doi = {10.1109/EMBC53108.2024.10781833},
	abstract = {Image-based risk models have the potential to aid in the identification of individuals who would benefit from cancer screening. However, models need to be robust against variations in image acquisition and reconstruction parameters, which alter the appearance of images and may lead to different downstream predictions. We evaluated Sybil, an imaging-based lung cancer model that predicts up to six-year risk, on a lung cancer screening dataset that was acquired and reconstructed using a range of parameters. Using raw projection data from 169 retrospectively acquired low-dose computed tomography (LDCT) scans, we generated six image conditions for each case, varying reconstruction kernels (smooth, medium, sharp) and slice thicknesses (1.0mm, 2.0mm). Each image condition was processed and run through the pre-trained Sybil model in the same way. Variations in predicted risk scores were observed across various kernels and slice thicknesses, suggesting that deep features derived from LDCT scans can be sensitive to nuanced variations in acquisition and reconstruction parameters. Our study underscores the importance of enhancing understanding of how technical parameters impact predictive models like Sybil to enhance the reliability of model outputs, facilitating more accurate and robust clinical decision support.},
	urldate = {2025-08-08},
	booktitle = {2024 46th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Zhuang, Luoting and Yadav, Anil and Kim, Grace H and Mohammad Hossein Tabatabaei, Seyed and Prosper, Ashley and Hsu, William},
	month = jul,
	year = {2024},
	note = {ISSN: 2694-0604},
	keywords = {Accuracy, Biological system modeling, Computational modeling, Computed tomography, Engineering in medicine and biology, generalizability, Image reconstruction, Kernel, lung cancer, Lung cancer, machine learning, Predictive models, Reliability},
	pages = {1--5},
	file = {Full Text PDF:/Users/luotingzhuang/Zotero/storage/4DBU6VDU/Zhuang et al. - 2024 - Exploring the Impact of Acquisition and Reconstruc.pdf:application/pdf},
}

@inproceedings{zhu_dart_2024,
	title = {{DART}: {Deformable} {Anatomy}-{Aware} {Registration} {Toolkit} for {Lung} {CT} {Registration} with {Keypoints} {Supervision}},
	shorttitle = {{DART}},
	url = {https://ieeexplore.ieee.org/abstract/document/10635326},
	doi = {10.1109/ISBI56570.2024.10635326},
	abstract = {Spatially aligning two computed tomography (CT) scans of the lung using automated image registration techniques is a challenging task due to the deformable nature of the lung. However, existing deep-learning-based lung CT registration models are not trained with explicit anatomical knowledge. We propose the deformable anatomy-aware registration toolkit (DART), a masked autoencoder (MAE)-based approach, to improve the keypoint-supervised registration of lung CTs. Our method incorporates features from multiple decoders of networks trained to segment anatomical structures, including the lung, ribs, vertebrae, lobes, vessels, and airways, to ensure that the MAE learns relevant features corresponding to the anatomy of the lung. The pretrained weights of the transformer encoder and patch embeddings are then used as the initialization for the training of downstream registration. We compare DART to existing state-of-the-art registration models. Our experiments show that DART outperforms the baseline models (Voxelmorph, ViT-V-Net, and MAE-TransRNet) in terms of target registration error of both corrField-generated keypoints with 17\%, 13\%, and 9\% relative improvement, respectively, and bounding box centers of nodules with 27\%, 10\%, and 4\% relative improvement, respectively. Our implementation is available at https://github.com/yunzhengzhu/DART.},
	urldate = {2025-08-08},
	booktitle = {2024 {IEEE} {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Zhu, Yunzheng and Zhuang, Luoting and Lin, Yannan and Zhang, Tengyue and Tabatabaei, Hossein and Aberle, Denise R and Prosper, Ashley E and Chien, Aichi and Hsu, William},
	month = may,
	year = {2024},
	note = {ISSN: 1945-8452},
	keywords = {Anatomy-Aware Pretraining, Brain modeling, Computational modeling, Computed tomography, Image registration, Lung, Lung image registration, Masked Autoencoder, Training, Transformers},
	pages = {1--5},
	file = {Full Text PDF:/Users/luotingzhuang/Zotero/storage/7QIX6H9J/Zhu et al. - 2024 - DART Deformable Anatomy-Aware Registration Toolki.pdf:application/pdf},
}

@article{zhuang_patient-level_2024,
	title = {Patient-level thyroid cancer classification using attention multiple instance learning on fused multi-scale ultrasound image features},
	volume = {2023},
	issn = {1942-597X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10785838/},
	abstract = {For patients with thyroid nodules, the ability to detect and diagnose a malignant nodule is the key to creating an appropriate treatment plan. However, assessments of ultrasound images do not accurately represent malignancy, and often require a biopsy to confirm the diagnosis. Deep learning techniques can classify thyroid nodules from ultrasound images, but current methods depend on manually annotated nodule segmentations. Furthermore, the heterogeneity in the level of magnification across ultrasound images presents a significant obstacle to existing methods. We developed a multi-scale, attention-based multiple-instance learning model which fuses both global and local features of different ultrasound frames to achieve patient-level malignancy classification. Our model demonstrates improved performance with an AUROC of 0.785 (p{\textless}0.05) and AUPRC of 0.539, significantly surpassing the baseline model trained on clinical features with an AUROC of 0.667 and AUPRC of 0.444. Improved classification performance better triages the need for biopsy.},
	urldate = {2025-08-08},
	journal = {AMIA Annual Symposium Proceedings},
	author = {Zhuang, Luoting and Ivezic, Vedrana and Feng, Jeffrey and Shen, Chushu and Radhachandran, Ashwath and Sant, Vivek and Patel, Maitraya and Masamed, Rinat and Arnold, Corey and Speier, William},
	month = jan,
	year = {2024},
	pmid = {38222341},
	pmcid = {PMC10785838},
	pages = {1344--1353},
}

@article{lipkova_artificial_2022,
	title = {Artificial intelligence for multimodal data integration in oncology},
	volume = {40},
	issn = {1535-6108, 1878-3686},
	url = {https://www.cell.com/cancer-cell/abstract/S1535-6108(22)00441-X},
	doi = {10.1016/j.ccell.2022.09.012},
	language = {English},
	number = {10},
	urldate = {2025-08-08},
	journal = {Cancer Cell},
	author = {Lipkova, Jana and Chen, Richard J. and Chen, Bowen and Lu, Ming Y. and Barbieri, Matteo and Shao, Daniel and Vaidya, Anurag J. and Chen, Chengkuan and Zhuang, Luoting and Williamson, Drew F. K. and Shaban, Muhammad and Chen, Tiffany Y. and Mahmood, Faisal},
	month = oct,
	year = {2022},
	pmid = {36220072},
	note = {Publisher: Elsevier},
	keywords = {AI in oncology, deep learning, deep learning in oncology, multimodal AI, multimodal fusion, multimodal integration},
	pages = {1095--1110},
	file = {Full Text PDF:/Users/luotingzhuang/Zotero/storage/PV35Q5IR/Lipkova et al. - 2022 - Artificial intelligence for multimodal data integr.pdf:application/pdf},
}

@inproceedings{zhuang_deep_2022,
	title = {Deep learning-based integration of histology, radiology, and genomics for improved survival prediction in glioma patients},
	volume = {12039},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12039/120390Z/Deep-learning-based-integration-of-histology-radiology-and-genomics-for/10.1117/12.2626318.full},
	doi = {10.1117/12.2626318},
	abstract = {Management of aggressive malignancies, such as glioma, is complicated by a lack of predictive biomarkers that could reliably stratify patients based on treatment outcome. The complex mechanisms driving glioma recurrence and treatment resistance cannot be fully understood without the integration of multiscale factors such as cellular morphology, tissue microenvironment, and macroscopic features of the tumor and the host tissue. We present a weakly-supervised, interpretable, multimodal deep learning-based model fusing histology, radiology, and genomics features for glioma survival predictions. The proposed framework demonstrates the feasibility of multimodal integration for improved survival prediction in glioma patients.},
	urldate = {2025-08-08},
	booktitle = {Medical {Imaging} 2022: {Digital} and {Computational} {Pathology}},
	publisher = {SPIE},
	author = {Zhuang, Luoting and Lipkova, Jana and Chen, Richard and Mahmood, Faisal},
	month = apr,
	year = {2022},
	pages = {120390Z},
}
